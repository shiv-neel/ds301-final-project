---
title: "HW10"
author: "Shiva Neelakantan"
studentid: 616916317
output: html_document
---

# Problem 1

```{r}
library(tensorflow)
library(keras)
library(caret)
library(class)
```


```{r}
load_mnist <- function() {
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  train <<- load_image_file('train-images-idx3-ubyte')
  test <<- load_image_file('t10k-images-idx3-ubyte')
  
  train$y <<- load_label_file('train-labels-idx1-ubyte')
  test$y <<- load_label_file('t10k-labels-idx1-ubyte')  
}


show_digit <- function(arr784, col=gray(12:1/12), ...) {
  image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
mnist = load_mnist()
```

## 1a

```{r}
df.train = data.frame(train$x, train$y)
head(df.train)
```

```{r}
index.train = sample(1:60000, 3000, replace=FALSE)
index.test = sample(1:10000, 100, replace=FALSE)
```

To find optimal k:
```{r}
trControl = trainControl(method='cv', number=10)
fit = train(train.y~.,
            method='knn',
            data=df.train[index.train, ], 
            tuneGrid=expand.grid(k = c(1, 5, 7, 9)),
            trControl=trControl,)
```

```{r}
fit
```

We can streamline the cross-validation process using the caret package's trainControl() function. We then use this CV to fit training models for when k = 1, 5, 7, and 9. The R output is shown below. 


3000 samples
784 predictors

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2700, 2701, 2700, 2699, 2700, 2700, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  1  1.247349  0.8222815  0.3256765
  5  1.118240  0.8483458  0.4644658
  7  1.138570  0.8426957  0.5114788
  9  1.161074  0.8361078  0.5449368

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 5.

Thus, *optimal k = 5*.

Implementing KNN on test set:
```{r}
k5 = knn(train$x[index.train,], test$x[index.test,], train$y[index.train], k=5)
knn.pred = k5
knn.actual = test$y[index.test]
```

Confusion Matrix + Misclassification Rate:
```{r}
conf.matrix = table(knn.pred, knn.actual)
conf.matrix
overall.misclassification.rate = 1 - mean(knn.pred == knn.actual)
overall.misclassification.rate
```
Misclassification rates for each label:
```{r}
errors = c()
for (i in 1:10) {
  errors = append(errors, (1 - (conf.matrix[i,i] / sum(conf.matrix[,i]))))
}
specific.missclassification.errors = data.frame(seq(0:9)-1, errors)
specific.missclassification.errors
```

## 1b

```{r}
library(MASS)
var(train$x[,1])
lda.fit = lda(train$y~., data=df.train, subset=index.train)
```
Applying LDA with this data set throws an error that some variables appear to be constant within groups. This is likely attributed to the fact that each pixel will not have the same variance. For instance, a pixel towards the borders of the 28x28 grid will always be completely dark, and thus, that predictor will have 0 variance. However, a pixel towards the center may have very disparate values across all the images, and thus, have a significant variance. 

One of LDA's assumptions is that variance must be constant across all predictors, but this data set cannot ensure that this assumption is met. Therefore, LDA would not be an optimal strategy to apply for this data set.

## 1c

First of all, because KNN is non-parametric, training the data is much faster, which is great for a data set of this magnitude and a computer with finite processing power.

Additionally, KNN offers us non-linear decision boundaries. This is much needed with a data set like this, because there are 10 different labels, and we need a flexible boundary to be able to draw effective decision boundaries. 

Furthermore, we only have one tuning parameter: k. This makes our model creation much simpler and quicker, because we just need to find an optimal k to implement, and we're good to go. We don't need to worry about constant variance or normal distribution assumptions as we would have to with a classifier like LDA.

# Problem 2

```{r}
load_fashion_mnist <- function() {
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  fashion.train <<- load_image_file('fashion/train-images-idx3-ubyte')
  fashion.test <<- load_image_file('fashion/t10k-images-idx3-ubyte')
  
  fashion.train$y <<- load_label_file('fashion/train-labels-idx1-ubyte')
  fashion.test$y <<- load_label_file('fashion/t10k-labels-idx1-ubyte')  
}


show_digit <- function(arr784, col=gray(12:1/12), ...) {
  image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
fashion_mnist = load_fashion_mnist()
```

## 2a

```{r}
fashion.train.2 = file('fashion/t10k-images-idx3-ubyte', 'rb')
m = matrix(readBin(fashion.train.2, integer(), size=1, n=28*28, endian='big'), 28,28)
m = image(m)
par(mfrow=c(1,1))
par(mar=c(0,0,0,0))
for(i in 1:5){
  m = matrix(readBin(fashion.train.2,integer(), size=1, n=28*28, endian='big'), 28, 28)
  image(m[,28:1])
}
```
These images appear to be pixelated images of different clothing/fashion garments. 


## 2b

```{r}
df.train.fashion = data.frame(fashion.train$x, fashion.train$y)
head(df.train.fashion)
```

```{r}
index.train = sample(1:60000, 3000, replace=FALSE)
index.test = sample(1:10000, 100, replace=FALSE)
```

```{r}
trControl = trainControl(method='cv', number=10)
fit = train(fashion.train.y~.,
            method='knn',
            data=df.train.fashion[index.train, ], 
            tuneGrid=expand.grid(k = c(1, 5, 7, 9)),
            trControl=trControl,)
```

```{r}
fit
```
We can streamline the cross-validation process using the caret package's trainControl() function. We then use this CV to fit training models for when k = 1, 5, 7, and 9. The R output is shown below. 

k-Nearest Neighbors 

3000 samples
 784 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2700, 2699, 2700, 2700, 2700, 2700, ... 
Resampling results across tuning parameters:

  k  RMSE      Rsquared   MAE      
  1  1.648786  0.7033693  0.6873648
  5  1.324519  0.7907049  0.7268323
  7  1.315044  0.7928778  0.7473595
  9  1.322577  0.7901167  0.7669972

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.

Thus, *optimal k = 7*.

Implementing KNN on test set:
```{r}
k7 = knn(fashion.train$x[index.train,], fashion.test$x[index.test,], fashion.train$y[index.train], k=7)
knn.pred = k7
knn.actual = test$y[index.test]
```

Confusion Matrix + Misclassification Rate
```{r}
conf.matrix = table(knn.pred, knn.actual)
conf.matrix
overall.misclassification.rate = 1 - mean(knn.pred == knn.actual)
overall.misclassification.rate
```

Misclassification rates for each label
```{r}
errors = c()
for (i in 1:10) {
  errors = append(errors, (1 - (conf.matrix[i,i] / sum(conf.matrix[,i]))))
}
specific.missclassification.errors = data.frame(seq(0:9)-1, errors)
specific.missclassification.errors
```
The confusion matrix, overall misclassification rate, and specific misclassification error rates are very similar to our discoveries in part (1a).

# Problem 3

## 3a

```{r}
spam = read.csv('C:/Users/shivn/Downloads/spambase.data', header=FALSE)
```

Reporting a meaningful email as spam *(false positive)* is more critical than reporting a spam email as meaningful (false negative). Not viewing a meaningful email due to it being marked as spam can, in the worst case, have significant consequences, where as the alternative is, at worst, just an annoyance or frustration. 

## 3b

```{r}
library(glmnet)
library(ROCR)
n = nrow(spam)
train = sample(1:n, n/2, replace=FALSE)
spam.train = spam[train, ]
spam.test = spam[-train, ]
glm.fit = glm(V58~., data=spam, subset=train, family='binomial')
glm.prob = predict(glm.fit, spam.test, type='response')

ROCRpred = prediction(glm.prob, spam.test$V58)W
plot(performance(ROCRpred,'tpr','fpr'), colorize=TRUE, 
     print.cutoffs.at=seq(0, 1, by=0.05), text.adj=c(-0.2, 1.7))
```
## 3c

```{r}
glm.pred = rep(1, length(-train))
glm.pred[glm.prob <= 0.5] = 0
conf.matrix = table(glm.pred, spam.test$V58)
conf.matrix

false.pos.rate = conf.matrix[2, 1] / length(-train)
false.pos.rate

false.neg.rate = conf.matrix[1, 2] / length(-train)
false.neg.rate
```
false positive rate: 0.02478
false negative rate: 0.04478

## 3d

```{r}
perf = performance(ROCRpred,'tpr','fpr')
thresholds = data.frame(threshold = perf@alpha.values[[1]],fpr = perf@x.values[[1]], tpr = perf@y.values[[1]])

fpr0.03 = subset(thresholds, fpr < 0.03)
tail(fpr0.03)
```
Setting the threshold to roughly 0.6 will yield a false positive rate just below 0.03.


