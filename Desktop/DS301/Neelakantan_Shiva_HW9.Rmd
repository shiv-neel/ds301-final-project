---
title: "Shiva_Neelakantan_HW9"
author: "Shiva Neelakantan"
date: '2022-04-18'
output: html_document
---

# Problem 1

## 1a

```{r}
misclassification.rate = (25+32)/(40+32+25+121)
misclassification.rate
```

I believe that a false negative (predicting someone does not have the virus when they actually do) is more troubling, because they will unknowingly continue to spread the virus to other people. Although the false positive is also inconvenient, as you would force someone to quarantine/hospitalize when they don't need to, it is the safer of the two errors. 

To decrease the likelihood of a false negative, we could lower the threshold, so that the classifier is more likely to classify a person as sick instead of healthy. This may increase the overall misclassification rate, but will also decrease the false positive rate, which is ultimately a safer option.

## 1b

```{r}
library(MASS)
library(caret)
library(class)
library(ISLR2)
```

## 1c

With a p as large as 10,000, there are many "dimensions" for the data. This means the data points are more likely to be spread out in space, because there are so many different predictors. The high space between points makes it difficult to accurately classify a point by its nearest neighbors.

For instance, some point's nearest 3 neighbors could be very close to that point for say, 5,000 of the predictors, but extremely far off for the other 5,000 predictors. This will lead to us incorrectly classifying much of the data. Thus, even though n is much greater than p, the extremely high number of dimensions will cause serious problems for KNN.

## 1d

i. Logistic Regression: Logically speaking, the average woman and man have different average height and weight. We can use this knowledge to set a threshold for the average weight and height of a woman vs a man to be somewhere between their respective averages for each category. Thus, our model will logistically classify a new value to be a woman if the weight and height lean closer to the averages for women, and vice-versa for men.

ii. LDA. Annual income and weekly working hours are continuous variables, and most likely have some sort of linear correlation. This is perfect for LDA, as we can most likely figure out a nice way to linearly partition the data into two categories.


iii. KNN. When the decision boundary is not easily modelable by a curve, it is better for us to classify points by its neighbors, rather than attempt to create a linear model that innacurately partitions the data.

## 1e

If the decision boundary is linear and the constant covariance assumption is true, I would expect LDA to perform better on the testing set. LDA has higher bias, and thus smaller variance. Therefore, LDA is less likely to overfit the training data, because of its low flexibility, and will thus perform better on the testing set than QDA. 

## 1f

QDA may perform better on the training set in comparison to LDA, because QDA is more likely to overfit the training data than LDA, due to its higher flexibility and lower bias. Thus, it may have higher accuracy with the training set than LDA. 

## 1g

P(x) is the prior probability of some predictor X. To get P(x), we just need to calculate the fraction of our data in which that predictor is true. Thus, we don't need to worry about estimating it, we can just directly calculate it from the data.

# Problem 2

## 2a

```{r}
set.seed(1)
X1 = rnorm(1000, 0, 0.9)
X2 = rnorm(1000, 1, 1)
X3 = rnorm(1000, 0, 2)

B0 = 1
B1 = 2
B2 = 3
B3 = 2

pr = exp(B0 + B1*X1 + B2*X2 + B3*X3) / (1 + exp(B0 + B1*X1 + B2*X2 + B3*X3))

Y = rbinom(1000, 1, pr)
df = data.frame(Y, X1, X2, X3)

train = sample(1:nrow(df),nrow(df)/2, replace=FALSE)
test = (-train)
```

## 2b

```{r}
glm.fit = glm(Y~., data=df, subset=train, family='binomial')
glm.prob = predict(glm.fit, df[test, ], type='response')
glm.pred = rep(1, length(test))
glm.pred[glm.prob <= 0.5] = 0
conf.matrix = table(glm.pred, df[test, ]$Y)
conf.matrix
misclassification.rate = (conf.matrix[1, 2] + conf.matrix[2, 1]) / nrow(df)
misclassification.rate
```

## 2c

```{r}
lda.fit = lda(Y~., data=df, subset=train)
lda.pred = predict(lda.fit, df)
conf.matrix2 = table(lda.pred$class, df$Y)
conf.matrix2
misclassification.rate2 = mean(lda.pred$class!=df$Y) 
misclassification.rate2
```

## 2d

```{r}
df2 = df[train, ]
scaled.df2 = scale(df2[, -1])

flds = createFolds(df2$Y, k=5, list=TRUE, returnTrain=FALSE)
K = c(1, 3, 5)

cv_error = matrix(NA, 5, 3)

for (j in 1:3) {
  k = K[j]
  for (i in 1:5) {
    test_idx = flds[[i]]
    testX = scaled.df2[test_idx, ]
    trainX = scaled.df2[-test_idx, ]
    
    trainY = df2$Y[-test_idx]
    testY = df2$Y[test_idx]
    
    knn.pred = knn(trainX, testX, trainY, k=k)
    cv_error[i, j] = mean(testY != knn.pred)
  }
}

apply(cv_error, 2, mean)

knn.pred = knn(df[train, ], df[test, ], df[train, ]$Y, k=5)
table(knn.pred, df[test, ]$Y)
mean(df[test, ]$Y != knn.pred)
```
## 2e

KNN had the lowest misclassification rate, of only 0.04. This was followed by LDA, with a rate of 0.048, and QDA, with a rate of 0.089.


# Problem 3

## 3a

```{r}
library(ISLR2)
```


```{r}
trainIdx = Weekly$Year <= 2008
test = Weekly[Weekly$Year > 2008, ]
lda.fit = lda(Direction~Lag2, data=Weekly, subset=trainIdx)
lda.fit
```

```{r}
test
```

Class specific means:
Down: -0.03568254
Up:    0.26036581

Prior Probabilities:
     Down        Up 
0.4477157 0.5522843

## 3b

```{r}
lda.pred = predict(lda.fit, Weekly[trainIdx, ])
Weekly[1, ]$Direction
lda.pred$posterior[1, ]
```
Predicted probabilities for first training observation:
     Down        Up
0.4269994 0.5730006

LDA assigns a higher probability to 'Up' than to 'Down' for the first training observation, even though the actual label of the first observation was 'Down'. Thus, it does not match what I observed.

## 3c

```{r}
lda.pred = predict(lda.fit, Weekly[-trainIdx, ])
Weekly[986, ]$Direction
lda.pred$posterior[1, ]
```
Predicted probabilities for first test observation:
     Down        Up 
0.4377018 0.5622982

LDA assigns a higher probability to 'Up' than to 'Down' for the first test observation, even though the actual label for the first observation was 'Down'. Similar to part (3b), this does not match what I observed.

## 3d

```{r}
ldafit = lda(Direction~Lag2, data=Weekly)
probs = predict(lda.fit, test)
preds = rep('Down', nrow(test))
preds[probs$class == 'Up'] = 'Up'
conf.matrix = table(preds, test$Direction)
conf.matrix
correct.predictions = (conf.matrix[1, 1] + conf.matrix[2, 2]) / nrow(test)
correct.predictions
```

## 3e

LDA assumes the following:
1. The data are normally distributed/shaped like a bell curve when plotted
2. Each class 1, 2,..., K has constant variance (constant sigma^2)

For the most part, this data does follow the two aforementioned assumptions. Almost all the predictors (excluding volume) have a normal distribution, and similar standard deviation (and therefore similar variance, since variance is proportional to sigma).

```{r}
hist(Weekly$Lag1, breaks=100)
hist(Weekly$Lag2, breaks=100)
hist(Weekly$Lag3, breaks=100)
hist(Weekly$Lag4, breaks=100)
hist(Weekly$Lag5, breaks=100)
hist(Weekly$Volume, breaks=100)
hist(Weekly$Today, breaks=100)
```

## 3f

```{r}
qda.fit = qda(Direction~Lag2, data=Weekly, subset=trainIdx)
probs = predict(qda.fit, test)
preds = rep('Down', nrow(test))
preds[probs$class == 'Up'] = 'Up'
conf.matrix2 = table(preds, test$Direction)
conf.matrix2
correct.predictions = (conf.matrix2[1, 2]) / nrow(test)
correct.predictions
```

QDA predicted all the test values to be 'Up'.

## 3g

```{r}
Weekly2 = Weekly[trainIdx, ]
scaled.Weekly2 = scale(Weekly2[, -1])

flds = createFolds(Weekly2$Direction, k=5, list=TRUE, returnTrain=FALSE)
K = c(1, 3, 5)

cv_error = matrix(NA, 5, 3)

for (j in 1:3) {
  k = K[j]
  for (i in 1:5) {
    test_idx = flds[[i]]
    testX = scaled.Weekly2[test_idx, ]
    trainX = scaled.Weekly2[-test_idx, ]
    
    trainY = Weekly2$Direction[-test_idx]
    testY = Weekly2$Direction[test_idx]
    
    knn.pred = knn(trainX, testX, trainY, k=k)
    cv_error[i, j] = mean(testY != knn.pred)
  }
}

apply(cv_error, 2, mean)

knn.pred = knn(Weekly[trainIdx, ], Weekly[-trainIdx, ], Weekly[trainIdx, ]$Direction, k=5)
table(knn.pred, Weekly[-trainIdx, ]$Direction)
mean(Weekly[-trainIdx, ]$Direction != knn.pred)
```
## 3h
Of these methods, LDA seems to work the best. This is most likely due to the fact that this data follows both assumptions of LDA nearly perfectly (the predictors are normally distributed, and have similar variance).





