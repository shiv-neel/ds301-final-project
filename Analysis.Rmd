---
title: "CBB_Analysis"
author: "Shiva Neelakantan, Ryan Scehovic, Ryan McNally, Saketh Jonnadula"
date: '2022-05-01'
output: html_document
---

```{r}
df = read.csv('./cbb.csv')
df[is.na(df)] <- -1
sum(is.na(df))
df$teamYear = paste(df$TEAM, df$YEAR)
head(df)
```

```{r}
library(leaps)
library(ggplot2)
library(VIM)
library(cluster)
library(dplyr)
library(ROCR)
library(MASS)
```

```{r}
df = df %>% mutate(WinRate = W/G)
for (i in 1:length(df$POSTSEASON)) {
  if (df$POSTSEASON[i] == 'Champions')
    df$POSTSEASON[i] = 1
  else if (df$POSTSEASON[i] == '2ND')
    df$POSTSEASON[i] = 1
  else if (df$POSTSEASON[i] == 'F4')
    df$POSTSEASON[i] = 1
  else if (df$POSTSEASON[i] == 'E8')
    df$POSTSEASON[i] = 1
  else if (df$POSTSEASON[i] == 'S16')
    df$POSTSEASON[i] = 1
  else if (df$POSTSEASON[i] == 'R32')
    df$POSTSEASON[i] = 1
  else if (df$POSTSEASON[i] == 'R64')
    df$POSTSEASON[i] = 1
  else df$POSTSEASON[i] = 0
}
df$SEED[is.na(df$SEED)] = 0
```

creating training and test sets
```{r}
set.seed(7)
trainIdx = sample(1:nrow(df),nrow(df)/2, replace=FALSE)
train = df[trainIdx,]
test = df[-trainIdx,]

```


```{r}
head(df)
hist(df$G, main="Number of games played")
hist(df$W, main="Number of games won")
hist(df$ADJOE, main="Adjusted Offensive Efficiency ")
hist(df$ADJDE, main="Adjusted Defensive Efficiency")
hist(df$BARTHAG, main="Power Rating")
hist(df$EFG_O, main="Effective Field Goal Percentage Shot")
hist(df$EFG_D, main="Effective Field Goal Percentage Allowed")
hist(df$TOR, main="Turnover Percentage Allowed (Turnover Rate)")
hist(df$TORD, main="Turnover Percentage Committed (Steal Rate)")
hist(df$ORB, main="Offensive Rebound Rate")
hist(df$DRB, main="Offensive Rebound Rate Allowed")
hist(df$FTR, main="Free Throw Rate")
hist(df$FTRD, main="Free Throw Rate Allowed")
hist(df$X2P_O, main="Two-Point Shooting Percentage")
hist(df$X2P_D, main="Two-Point Shooting Percentage Allowed")
hist(df$X3P_O, main="Three-Point Shooting Percentage")
hist(df$X3P_D, main="Three-Point Shooting Percentage Allowed")
hist(df$ADJ_T, main="Adjusted Tempo")
hist(df$WAB, main="Wins Above Bubble")
hist(df$SEED, main="Tournament Seed")
hist(df$WinRate, main="Win Rate")
```

# Q1 SETUP
```{r}
set.seed(7)
df2 <- df[5:22]
df2$WinRate=df$WinRate
df2$POSTSEASON=as.numeric(df2$POSTSEASON)
df2$SEED=df$SEED
train_index = sample(1:nrow(df2),nrow(df2)/4, replace=FALSE)
test_index = -train_index
train = df2[train_index,]
test = df2[-train_index,]
```


#Q1 logistic regression assumption test
```{r}
cor(df2[,c('ADJOE', 'EFG_O',
            'TOR',  'ORB', 
            'FTR', 'X2P_O', 'X3P_O', 'ADJ_T')])
cor(df2[,c('ADJDE','TORD','DRB', 'FTRD','X2P_D', 'EFG_D','X3P_D')])
#'EFG_D',
```

# Q1 logistic regression - offense + defense
```{r}
library(MASS)
glm.fit = glm(POSTSEASON~ADJOE+ADJDE+TOR+TORD+ORB+DRB+FTR+FTRD+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T, data=df2, subset=train_index, family='binomial')
summary(glm.fit)
glm.prob = predict(glm.fit, test,type='response')
glm.pred = rep(0,nrow(test))
glm.pred[glm.prob >0.5] = 1
table(glm.pred,test$POSTSEASON)
1-mean(glm.pred == test$POSTSEASON)
cMatrix=table(glm.pred,test$POSTSEASON)
falsenegRate = cMatrix[1,2] / nrow(test)
falsenegRate
falseposRate = cMatrix[2,1] / nrow(test)
falseposRate
ROCRpred <- prediction(glm.prob,test$POSTSEASON)
plot(performance(ROCRpred,'tpr','fpr'))
plot(performance(ROCRpred,'tpr','fpr'),colorize=TRUE,
     print.cutoffs.at=seq(0,1,by=0.05), text.adj=c(-0.2,1.7))
```

# Q1 logistic regression - offense only
```{r}
#ADJOE+ADJDE+EFG_O+EFG_D+TOR+TORD+ORB+DRB+FTR+FTRD+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T
glm.fit = glm(POSTSEASON~ADJOE+TOR+ORB+FTR+X2P_O+X3P_O+ADJ_T, data=df2, subset=train_index, family='binomial')
summary(glm.fit)
glm.prob = predict(glm.fit, test,type='response') 
glm.pred = rep(0,nrow(test))
glm.pred[glm.prob >0.5] = 1
table(glm.pred,test$POSTSEASON) #rows are predicted, # columns are true 
1-mean(glm.pred == test$POSTSEASON)
cMatrix=table(glm.pred,test$POSTSEASON)
falsenegRate = cMatrix[1,2] / nrow(test)
falsenegRate
falseposRate = cMatrix[2,1] / nrow(test)
falseposRate
ROCRpred <- prediction(glm.prob,test$POSTSEASON)
plot(performance(ROCRpred,'tpr','fpr'))
plot(performance(ROCRpred,'tpr','fpr'),colorize=TRUE,
     print.cutoffs.at=seq(0,1,by=0.05), text.adj=c(-0.2,1.7))
```

# Q1 logistic regression - defense only
```{r}
#ADJOE+ADJDE+EFG_O+EFG_D+TOR+TORD+ORB+DRB+FTR+FTRD+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T
glm.fit = glm(POSTSEASON~ADJDE+TORD+DRB+FTRD+X3P_D, data=df2, subset=train_index, family='binomial')
summary(glm.fit)
glm.prob = predict(glm.fit, test,type='response') 
glm.pred = rep(0,nrow(test))
glm.pred[glm.prob >0.5] = 1
table(glm.pred,test$POSTSEASON)
1-mean(glm.pred == test$POSTSEASON)
cMatrix=table(glm.pred,test$POSTSEASON)
falsenegRate = cMatrix[1,2] / nrow(test)
falsenegRate
falseposRate = cMatrix[2,1] / nrow(test)
falseposRate
ROCRpred <- prediction(glm.prob,test$POSTSEASON)
plot(performance(ROCRpred,'tpr','fpr'))
plot(performance(ROCRpred,'tpr','fpr'),colorize=TRUE,
     print.cutoffs.at=seq(0,1,by=0.05), text.adj=c(-0.2,1.7))
```


# Q1 - Testing for LDA Assumptions
```{r}
#ADJOE+ADJDE+EFG_O+EFG_D+TOR+TORD+ORB+DRB+FTR+FTRD+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T
CovOff <- cbind(df2$ADJOE, df2$ADJDE, df2$EFG_O, df2$EFG_D, df2$TOR, df2$TORD, df2$ORB, df2$DRB, df2$FTR, df2$FTRD,
                   df2$X2P_O, df2$X2P_D, df2$X3P_O, df2$X3P_D, df2$ADJ_T)


cov(CovOff)

```

# Q1 LDA - offense + defense
```{r}
lda.fit = lda(POSTSEASON~ADJOE+ADJDE+EFG_O+EFG_D+TOR+TORD+ORB+DRB+FTR+FTRD+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T, data=df2, subset=train_index)
summary(lda.fit)
lda.pred = predict(lda.fit,test)
table(lda.pred$class,test$POSTSEASON)
mean(lda.pred$class==test$POSTSEASON)
```


# Q1 LDA - offense only
```{r}
lda.fit = lda(POSTSEASON~ADJOE+EFG_O+TOR+ORB+FTR+X2P_O+X3P_O+ADJ_T, data=df2, subset=train_index)
summary(lda.fit)
lda.pred = predict(lda.fit,test)
table(lda.pred$class,test$POSTSEASON)
mean(lda.pred$class==test$POSTSEASON)
```

# Q1 LDA - offense only - multiple train/test splits
```{r}

ldaAccuracy = c()
qdaAccuracy = c()


for(i in 1:10){
  train_index = sample(1:nrow(df2),nrow(df2)/4, replace=FALSE)
  test_index = -train_index
  train = df2[train_index,]
  test = df2[-train_index,]
  lda.fit = lda(POSTSEASON~ADJOE+EFG_O+TOR+ORB+FTR+X2P_O+X3P_O+ADJ_T, data=df2, subset=train_index)
  lda.pred = predict(lda.fit,test)
  ldaAccuracy[i] = mean(lda.pred$class==test$POSTSEASON)
  
  qda.fit = qda(POSTSEASON~ADJOE+EFG_O+TOR+ORB+FTR+X2P_O+X3P_O+ADJ_T, data=df2, subset=train_index)
  qda.pred = predict(qda.fit,test)
  qdaAccuracy[i] = mean(qda.pred$class==test$POSTSEASON)  
}
ldaAccuracy
qdaAccuracy

# qda always 0.5-2% worse than lda

```



# Q1 LDA - defense only
```{r}
lda.fit = lda(POSTSEASON~ADJDE+EFG_D+TORD+DRB+FTRD+X2P_D+X3P_D, data=df2, subset=train_index)
summary(lda.fit)
lda.pred = predict(lda.fit,test)
table(lda.pred$class,test$POSTSEASON)
mean(lda.pred$class==test$POSTSEASON)
```

# Q1 LDA - defense only-  multiple train/test splits
```{r}

ldaAccuracy = c()
qdaAccuracy = c()


for(i in 1:10){
  train_index = sample(1:nrow(df2),nrow(df2)/4, replace=FALSE)
  test_index = -train_index
  train = df2[train_index,]
  test = df2[-train_index,]
  lda.fit = lda(POSTSEASON~ADJDE+EFG_D+TORD+DRB+FTRD+X2P_D+X3P_D, data=df2, subset=train_index)
  lda.pred = predict(lda.fit,test)
  ldaAccuracy[i] = mean(lda.pred$class==test$POSTSEASON)
  
  qda.fit = qda(POSTSEASON~ADJDE+EFG_D+TORD+DRB+FTRD+X2P_D+X3P_D, data=df2, subset=train_index)
  qda.pred = predict(qda.fit,test)
  qdaAccuracy[i] = mean(qda.pred$class==test$POSTSEASON)  
}
ldaAccuracy
qdaAccuracy

# qda always 1-2% worse than lda
```


# Q2 -- Subset Selection
```{r}
best.subsets = regsubsets(WinRate~ADJOE+ADJDE+TOR+TORD+ORB+DRB+FTR+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T, data=df, nvmax=24)
subsets.summary = summary(best.subsets)
subsets.summary
n = dim(df)[1]
p = rowSums(subsets.summary$which)
best.rss = subsets.summary$rss
best.aic = n * log(best.rss/n) + 2*p
best.bic = n * log(best.rss/n) + p*log(n)
plot(best.aic)
plot(best.bic)
best.aic
best.bic
best.rss
```

# Q2 -- MLR using best subset
```{r}
mlr.model = lm(WinRate~ADJDE+TOR+TORD+ORB+DRB+FTR+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T,data=train)
model.summary = summary(mlr.model)
model.summary
mean((test$WinRate - predict.lm(mlr.model, test)) ^ 2)
```

```{r}
library(glmnet)
x = model.matrix(WinRate~ADJOE+ADJDE+EFG_O+EFG_D+TOR+TORD+ORB+DRB+FTR+FTRD+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T,data=df)[,-1] 
Y = train$WinRate
grid = 10^seq(10,-2,length=100)
cv.out.ridge = cv.glmnet(x[trainIdx,],Y,alpha = 0, lambda = grid) 
plot(cv.out.ridge)
bestlambda = cv.out.ridge$lambda.min
bestlambda
#really small lambda means a very flexible model.
ridge.train = glmnet(x[trainIdx,],Y,alpha=0,lambda=grid)
ridge.pred = predict(ridge.train,s=bestlambda,newx=x[-trainIdx,])
mean((ridge.pred-Y[-trainIdx])^2)
final.ridge = glmnet(x[trainIdx,],Y,alpha=0,lambda=bestlambda)
coef(final.ridge)

plot(ridge.train,xvar="lambda",label=TRUE)
```


# Q2 -- Ridge Regression
```{r}
library(glmnet)
x = model.matrix(WinRate~ADJOE+ADJDE+TOR+TORD+ORB+DRB+FTR+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T,data=df)[,-1] 
#x= model.matrix(WinRate~.,data=df2)[,-1]
Z = df2[trainIdx,]$WinRate
grid = 10^seq(10,-2,length=100)
cv.out.ridge = cv.glmnet(x[trainIdx,],Z,alpha = 0, lambda = grid) 
plot(cv.out.ridge)
bestlambda = cv.out.ridge$lambda.min
bestlambda
#really small lambda means a very flexible model.
ridge.train = glmnet(x[trainIdx,],Z,alpha=0,lambda=grid)
ridge.pred = predict(ridge.train,s=bestlambda,newx=x[-trainIdx,])
mean((ridge.pred-Z[-trainIdx])^2)
final.ridge = glmnet(x[trainIdx,],Z,alpha=0,lambda=bestlambda)
coef(final.ridge)

plot(ridge.train,xvar="lambda",label=TRUE)
```
# Q2 -- Lasso
```{r}
library(glmnet)
x = model.matrix(WinRate~ADJOE+ADJDE+TOR+TORD+ORB+DRB+FTR+X2P_O+X2P_D+X3P_O+X3P_D+ADJ_T,data=df2)[,-1] 
#x= model.matrix(WinRate~.,data=df2)[,-1]
Z = df2[trainIdx,]$WinRate
grid = 10^seq(10,-2,length=100)
cv.out.lasso = cv.glmnet(x[trainIdx,],Z,alpha = 1, lambda = grid) 
plot(cv.out.lasso)
bestlambda2 = cv.out.lasso$lambda.min
bestlambda2
#really small lambda means a very flexible model.
lasso.train = glmnet(x[trainIdx,],Z,alpha=1,lambda=grid)
lasso.pred = predict(lasso.train, s=bestlambda2,newx=x[-trainIdx,])
mean((lasso.pred-Z[-trainIdx])^2)
final.lasso = glmnet(x[trainIdx,],Z,alpha=1,lambda=bestlambda2)
coef(final.lasso)
plot(lasso.train,xvar="lambda",label=TRUE)
```
